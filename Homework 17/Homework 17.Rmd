---
title: "HW #17 Computer Intensive Statistics in Ecology"
author: "Ä¬¥D©¶ Eric Su"
date: "2017/6/22"
output: html_document
---
[Click here for other works of the author on RPubs](http://rpubs.com/prorichter)

In this assignment, we build a classification model using multi-layer perceptron.

### Load packages
```{r}
library(knitr)
library(ggplot2)
```

## 1. Use the data provided (modeldata.txt). The first two columns are $x_1$ and $x_2$, the column 3 to 5 represent codings for three class (y).


### Load data
```{r}
data <- read.table("modeldata.txt")
colnames(data) <- c("X1", "X2", "Group1", "Group2", "Group3")

#show a part of the data
kable(head(data), digits = 4)
```

### Visualize data
```{r}
#plot data according to group
p_data <- data.frame(data[, 1:2], apply(data[, 3:5], 1, which.max))
colnames(p_data) <- c("X1", "X2", "Group")
p_data$Group = factor(p_data$Group)
ggplot(p_data, aes(x = X1, y = X2, col = Group))+
    geom_point(alpha = 0.8, size = 2) +
    theme_bw() +
    labs(x = expression(x[1]), y = expression(x[2])) +
    coord_fixed(ratio = 1) +
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```


## 2. Write your own MLP. Use off-line learning, learning rate = 0.001, learning time = 1000 step, # of hidden neuron = 5, use tanh as your activation function. Plot MSE vs learning steps. Calculate the min(MSE) and associated optimal weights. (NOTE: you should try different initial conditions several times to check whether you get trapped in the local minimum.)

```{r}
#seperate data into training and test sets
train = data[1:500, ]
test = data[501:600, ]
```

```{r}
tanh <- function(x){
    (exp(x) - exp(-x)) / (exp(x) + exp(-x))
}

predict.dnn <- function(model, data) {
  # new data, transfer to matrix
  new.data <- data.matrix(data)
  
  # Feed Forward
  hidden.layer <- sweep(new.data %*% model$w1 ,2, model$b1, '+')
  # neurons : Rectified Linear
  hidden.layer <- pmax(hidden.layer, 0)
  score <- sweep(hidden.layer %*% model$w2, 2, model$b2, '+')
  
  # Loss Function: softmax
  score.exp <- exp(score)
  probs <- sweep(score.exp, 1, rowSums(score.exp), '/') 
  
  # select max possiblity
  labels.predicted <- max.col(probs)
  return(labels.predicted)
}

ANN <- function(iv_num, dv_num, trainset = data, testset = NULL, hidden = 5, maxstep = 1000, learn_rate = 0.001, reg = 1e-3){
    
    n <- nrow(trainset)
    
    # extract the data and label
    # don't need atribute 
    X <- unname(data.matrix(trainset[, iv_num]))
    Y <- trainset[, dv_num]

    # create index for both row and col
    Y.index <- cbind(1:n, Y)
    
    # number of input features
    D <- ncol(X)
    # number of categories for classification
    
    # create and init weights and bias 
    w1 <- matrix(runif(D * hidden, -0.5, 0.5), nrow = D, ncol = hidden)
    b1 <- matrix(runif(hidden, -0.5, 0.5), nrow = 1, ncol = hidden)
    
    w2 <- matrix(runif(hidden *dv_num, -0.5, 0.5), nrow = hidden, ncol = dv_num)
    b2 <- matrix(runif(dv_num, -0.5, 0.5), nrow = 1, ncol = dv_num)
    
    # Training the network
    i <- 0
    while(i < maxstep){
        
        # iteration index
        i <- i + 1
        
        # forward ....
        # 1 indicate row, 2 indicate col
        hidden.layer <- sweep(X %*% w1 , 2, b1, '+')
        
        # neurons : ReLU
        hidden.layer <- pmax(hidden.layer, 0)
        hidden_pred = tanh(hidden.layer)
        score <- sweep(hidden_pred %*% w2, 2, b2, '+')
        pred <- tanh(score)
        error = trainset[, dv_num] - pred
        
        
        # backward ....
        dscores <- probs
        dscores[Y.index] <- dscores[Y.index] -1
        dscores <- dscores / n
        
        
        dw2 <- t(hidden.layer) %*% dscores 
        db2 <- colSums(dscores)
        
        dhidden <- dscores %*% t(w2)
        dhidden[hidden.layer <= 0] <- 0
        
        dw1 <- t(X) %*% dhidden
        db1 <- colSums(dhidden) 
        
        delta_j =  t((1 - tanh(score) ^ 2)) %*% as.matrix(error)
        a = delta_j %*% t(w2)
        a = colSums(a)
        # update ....
        w1 <- w1 - learn_rate * t((1 - tanh(hidden.layer) ^ 2) %*% a) %*% X
        b1 <- b1 - learn_rate * t((1 - tanh(hidden.layer) ^ 2) %*% a) %*% X
        
        w2 <- w2 - learn_rate * delta_j %*% hidden_pred
        b2 <- b2 - learn_rate * delta_j %*% hidden_pred
        
    }
    
    # final results
    # creat list to store learned parameters
    # you can add more parameters for debug and visualization
    # such as residuals, fitted.values ...
    model <- list(w1 = w1, b1 = b1, w2 = w2, b2 = b2)
    
    return(model)
}

```


```{r}
model <- ANN(iv_num = 1:2, dv_num = 3:5, trainset = train, testset = test, hidden = 5, maxstep = 2000)

```


```{r}
labels.dnn <- predict.dnn(model, p_data[, 1:2])

# 4. verify the results
table(p_data[, 3], labels.dnn)

#accuracy
mean(as.integer(p_data[, 3]) == labels.dnn)
```

in progress...


## Appendix - packages

## Part 1: neuralnet package

The greatest strength of the neuralnet package is it allows for us to plot the neural network model to examine it visually.

### Use package `neuralnet` to build neural network model
```{r}
library(neuralnet)

nn <- neuralnet(Group1 + Group2 + Group3 ~ X1 + X2, data = data, hidden = 5)
```

#show results
```{r}
#plot with weights
plot(nn)

pred_nn <- compute(nn, data[, 1:2])

table(p_data$Group, apply(pred_nn$net.result, 1, which.max))
```

### Prettier plot
```{r}
library(NeuralNetTools)
plotnet(nn, pos_col = "red", neg_col = "blue", alpha_val = 0.5)
```

Red lines indicates positive weights, blue for negative weights.

## Part 2: H2O package

Created for buliding machine learning models, the `H2O` package offers users greater computing power by connecting to another remote machine. Neural network models often requires immense computing power, thus the package is very useful when building such models.

### Build neural network model using `H2O` package
```{r results='hide', message=FALSE}
#load h2o package and connect to the ip it provides
library(h2o)

#connect to h2o server
h2o.init(nthreads = -1)
    
#train a neural network model for classification
model = h2o.deeplearning(y = "Group", training_frame = as.h2o(train), validation_frame = as.h2o(test), activation = "Rectifier", hidden = 5, epochs = 6000, train_samples_per_iteration = -2, export_weights_and_biases = T, nfolds = 10, fold_assignment = "Stratified")

#predict groups using our neural network model
pred = h2o.predict(model, as.h2o(p_data[, 1:2]))

#weights of nn model
weight1 = as.data.frame(t(h2o.weights(model, 1)))
rownames(weight1) <- c("X1", "X2")
colnames(weight1) <- c("Hidden 1", "Hidden 2", "Hidden 3", "Hidden 4", "Hidden 5")
kable(weight1, digits = 4, caption = "Weights of first layer")

weight2 = as.data.frame(t(h2o.weights(model, 2)))
rownames(weight2) <- c("Hidden 1", "Hidden 2", "Hidden 3", "Hidden 4", "Hidden 5")
colnames(weight2) <- c("Class 1", "Class 2", "Class 3")
kable(weight2, digits = 4, caption = "Weights of second layer")
```

### Confusion matrix for training set
```{r}
conf_train <- table(train[, 3], as.data.frame(pred)$predict[1:500])
conf_train
```

The overall accuracy of our predictions in the training set is `r sum(diag(conf_train)) / sum(conf_train)`.

### Confusion matrix for test set
```{r}
conf_test <- table(test[, 3], as.data.frame(pred)$predict[501:600])
conf_test
```

The overall accuracy of our predictions in the test set is `r sum(diag(conf_test)) / sum(conf_test)`.

### Accuracy diagnosis using 10-fold cross validation
```{r}
#10-fold cross validation
model@model$cross_validation_metrics_summary
```

### Plot error rate according to epoch for training and test sets
Test set is named "validation" in the diagram
```{r}
#plot epoch according to classification error
plot(model)
```

### Prepare data for plotting
```{r results='hide'}
grid <- as.data.frame(expand.grid(seq(min(data[, 1]), max(data[, 1]), length = 1000), seq(min(data[, 2]), max(data[, 2]), length = 1000)))
colnames(grid) <- c("X1", "X2")
pred <- h2o.predict(model, as.h2o(grid))
pred <- as.data.frame(pred)$predict
```

### Plot the decision boundary of our neural network model
```{r}
ggplot() +
    geom_raster(aes(x = grid[, 1],y = grid[, 2], fill = pred), alpha = 0.3, show.legend = F) +
    theme_bw() +
    geom_point(data = p_data, aes(x = X1, y = X2, color = Group), size = 2) + 
    labs(title = "Neural network decision boundary", x = expression(x[1]), y = expression(x[2])) +
    theme(panel.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank())
```

```{r echo=FALSE, results='hide'}
#disconnect from h2o server
h2o.shutdown(prompt = F)
```