---
title: "HW #8 Computer Intensive Statistics in Ecology"
author: "Ä¬¥D©¶ Eric Su"
date: "2017/4/16"
output: html_document
---
[Click here for other works of the author on RPubs](http://rpubs.com/prorichter)

### Load packages
```{r message = FALSE}
library(knitr)
library(fpc)
library(vegan)
library(cluster)
library(purrr)
library(factoextra)
library(pander)
```

## Q. Use the ¡§dominant¡¨ copepod species data (from HW1). Perform cluster analysis of stations based on percent composition data of the dominant species and tell your story about these copepod data.

For this assignment, **k-means clustering** and **hierarchical clustering** will be performed to analyze the structure of the stations according to dominant copepode species.

### Import data
```{r}
#import copepod data
copdata = read.table("copepod_composition.txt", header = T)

#import species name
species_name <- read.table("copepodSPlist.txt", sep = "\t")

#assign copepod species name
rownames(copdata) <- as.character(unlist(species_name))
```

### Extract dominant species data
Dominant species are defined as species with a percentage of 2% or more in any given station.
```{r}
#convert species frequency into percentage
copdata = copdata / 100

#find dominant species
dom = apply(copdata >= 0.02, 2, which)
dom = sort(unique(unlist(dom)))
dom = t(copdata[dom, ])

#show a part of the data
kable(dom[1:6, 1:6], caption = "Station data with percent composition of dominant species (partial)")
```

### Compute distance matrix with differnt distance measures
Using distance measures including *euclidean*, *manhattan*, *Bray-Curtis* and *Jaccard* to calculate distance matrix.
```{r}
dist_meth = c("euclidean", "manhattan", "bray", "jaccard")
dist_m = map(as.list(dist_meth), vegdist, x = dom)
```

### Determine the optimal number of clusters to use
Define function `find_k` for plotting scree plot to show how within groups sum of square (*WSS*) and average Silhouette width (*Si*) change with different number of clusters used.
```{r}
find_k <- function(data, plot_title = ""){
    #create data for within group sum of square
    wss = numeric(15)
    for (k in 1:15) wss[k] <- kmeans(data, k, nstart = 20)$tot.withinss
    
    #create data for average silhouette distance
    asw <- numeric(15)
    for (k in 2:15) asw[k] <- pam(data, k)$silinfo$avg.width
    
    #create s cree plot
    par(mar=c(5, 4, 4, 6))
    plot(1:15, wss, type = "b", main = plot_title, xlab = "Number of Clusters", ylab = "Within groups sum of squares")
    par(new = T)
    plot(1:15, asw, type = "l", lty = 2, col = "red", axes = F, xlab = NA, ylab = NA)
    axis(side = 4)
    mtext("Average Silhouette width", side = 4, line = 3)
    legend("topright", legend = c("WSS", "Si"), lty = c(1,2), col = c("black", "red"))
}
```

### Scree plots {.tabset}
Define plot titles.
```{r}
dist_meas = c("Euclidean", "Manhattan", "Bray-Curtis", "Jaccard")
```

Create scree plots to determine the number of clusters to use (k). Click on the tabs to show results for different distance methods.
```{r results = "asis"}
for(i in 1:4){
    cat("\n")
    cat("#### ", dist_meas[i], "\n")
    find_k(dist_m[[i]], plot_title = dist_meas[i])
    cat("\n")
}
```

###

From the result of scree plots, 2 clusters seems to be the most reasonable choice when *euclidean distance* is used and 3 clusters is optimal if other distance methods are used. In order to compare the effects of different distance measures, 3 clusters will be used in later analysis.

### Perform k-means clustering to analyze the clusters of stations
```{r}
#conduct k-means to find clusters
km = map(dist_m, pam, k = 3)

# extract cluster data
km_result = transpose(km)
km_cluster = as.data.frame(km_result$cluster)
colnames(km_cluster) <- dist_meas
```

### Silhoulette plots: results for k-means clustering {.tabset}
Plot the Silhouette plots for k-means clustering. Click on the tabs to see results for different distance methods.
```{r results='asis'}
for(i in 1:4){
    cat("\n")
    cat("#### ", dist_meas[i], "\n")
    plot(silhouette(km[[i]], dist_m[[i]]), main = paste("Silhoulette plot using ", dist_meas[i], " distance") , col = 1:3, border = NA, cex = 0.6)
    cat("\n")
}
```

### 

No significant differences exists between the clusters when different distance measures are used. However, it should be noted that stations p23 and wC are assigned to a different cluster when *euclidean distance* is used comparing with other methods.

### Clusters stability: k-means{.tabset}
Using the bootstrap method, assess the stability of clusters when using k-means. Click on different tabs to see results for different distance methods.
```{r results='asis'}
for(i in 1:4){
    cat("\n")
    cat("#### ", dist_meas[i], "\n")
    stab = clusterboot(dist_m[[i]], B = 1000, bootmethod = "boot", clustermethod = claraCBI, k = 3, count = F)
    stab_results = cbind(stab$bootmean, stab$bootbrd, stab$bootrecover)
    print(kable(stab_results, col.names = c("Clusterwise Jaccard bootstrap mean", "dissolved", "recovered"), caption = "Cluster stability assessment for k-means"))
    cat("\n")
}
```

###

Based on stability assessments, it seems that using *euclidean distance* would result in unstable clusters. Other distance methods all yield reasonably stable clusters.

### Perform hierarchical clustering to analyze the clusters of stations
```{r}
#perform hierarchical clustering on different distance measures and linkage methods
hc_agg = list()
m1 = list(method = list("single", "complete", "average", "ward"))
m2 = list(method = list("single", "complete", "average", "ward.D"))
for(i in 1:4){
    hc_agg[[i]] = pmap(m1, agnes, x = dist_m[[i]])
}
```

### Dendrogram: results for hierarchical clustering {.tabset .tabset-pills}
Define plot titles
```{r}
link_meth = c("Single", "Complete", "Average",  "Ward's")
```

Plot the dendrograms for hierarchical clustering. Click on different tabs to see results for different distance methods with different linkage methods.
```{r results='asis'}
for(i in 1:4){
    cat("#### ", dist_meas[i], "{.tabset results='asis'}", "\n")
    for(j in 1:4){
        cat("\n")
        cat("##### ", link_meth[j], "\n")
        print(fviz_dend(hc_agg[[i]][[j]], cex = 0.5, k = 3, lwd = 0.8, main = paste(link_meth[j], "linkage Dendrogram"), horiz = T, ylab = paste("Height", "\n", "\n", "Agglomerative coefficient=", round(hc_agg[[i]][[j]]$ac, 2), "\n", "Cophenetic correlation=", round(cor(dist_m[[i]], cophenetic(hc_agg[[i]][[j]])), 2))))
        cat("\n")
    }
    cat("\n")
}
```

###

From the dendrograms, we can see that using *euclidean distance* or *single linkage*  would give large clusters with many stations and small clusters with only a few stations (sometimes only 1) and have a chaining effect. This is undesirable for our analysis. Additionally, *single linkage* generally result in a lower cophenetic correlation while *Ward's linkage* yields a higher agglomerative coefficient.

### Clusters stability: hierarchical clustering {.tabset .tabset-pills}
Using the bootstrap method, assess the stability of clusters when using hierarchical clustering. Click on different tabs to see results for different distance methods with different linkage methods.
```{r results='asis'}
for(i in 1:4){
    cat("#### ", dist_meas[i], "{.tabset results='asis'}", "\n")
    for(j in 1:4){
        cat("\n")
        cat("##### ", link_meth[j], "\n")
        stab = clusterboot(dist_m[[i]], B = 1000, bootmethod = "boot", clustermethod = hclustCBI, k = 3, method = m2[[1]][[j]], count = F)
        stab_results = cbind(stab$bootmean, stab$bootbrd, stab$bootrecover)
        print(kable(stab_results, col.names = c("Clusterwise Jaccard bootstrap mean", "dissolved", "recovered"), caption = "Cluster stability assessment for hierarchical clustering"))
        cat("\n")
    }
    cat("\n")
}
```

###

According to the stability assessment, using *single linkage* and *complete linkage* generally yields unstable clusters. On the other hand, *Ward's distance* gives the most stable clusters.

## Cluster analysis result
The result of our analysis vary significantly with different distance measure and linkage methods. As mentioned above, using *euclidean distance* or *single linkage*  yields large clusters with many stations and small clusters with only a few stations. Consequently, *euclidean distance* and *single linkage* are inappropriate for analyzing the clusters of stations in this research.

All other methods return similar clustering results and also have similar performances on the *agglomerative coefficient*, *Cophenetic correlation* and *stability assessment*. Among other methods, both *Bray-Curtis* and *Jaccard distance* along with *Ward's linakge* have better performance in the three criterions with Jaccard distance method giving slightly more stable clusters. Therefore, the results from *Jaccard distance* and *Ward's linkage* will be used for further analysis.

### Report stations in different group
```{r}
#extract indice for cluster from result of heirarchical clustering using Jaccard distance and Ward's linkage method
clust = cutree(hc_agg[[4]][[4]], 3)

#subset station data according to the clusters
g = list()
clust_name = c()
for(i in 1:3){
    g[[i]] = subset(dom, clust == i)
    clust_name[i] = paste(rownames(g[[i]]), collapse = " ")
}

#display the clusters
clust_name = data.frame(clust_name)
colnames(clust_name) <- "Stations"
rownames(clust_name) <- c("Group 1", "Group 2", "Group 3")
pander(clust_name, split.cell = 35)
```

Based on the results of cluster analysis, it is evident that the groups of stations are highly related to the season in which copepod data are collected. Since groupings are based on the composition of different copepod species, it is very likely that season have a strong influence on the composition of copepod species. 

### Compare copepod composition in the 3 clusters
```{r}
#compute average composition percentage for every copepod for each group
g_mean = map(g, apply, MARGIN = 2, FUN = mean)
kable(data.frame(g_mean), col.names = c("Group 1 (Spring)", "Group 2 (Summer)", "Group 3 (Winter)"))

#extract copepod species that are prominant in each group (season). Prominant species are those with a higher composition percentage than the sum of composition percentage in other groups
dom_species_name <- rownames(data.frame(g_mean))
dom_g1 = dom_species_name[g_mean[[1]] > g_mean[[2]] + g_mean[[3]]]
dom_g2 = dom_species_name[g_mean[[2]] > g_mean[[1]] + g_mean[[3]]]
dom_g3 = dom_species_name[g_mean[[3]] > g_mean[[1]] + g_mean[[2]]]
```

As we can see, copepod species like *`r dom_g1`* are more prominent in **spring**, copepod species like *`r dom_g2`* are more prominent in **summer**, and copepod species such as *`r dom_g3`* are more prominent in **winter**.

It is therefore very likely that different season have significant impact on the abundance of these copepod species. In the next section, we would look deeper into the variables that may be affected by different seasons.

### Load and extract environmental data for stations in each season 
```{r}
envdata = read.table("enviANDdensity.txt", header = T)
rownames(envdata) <- envdata[, 1]
envdata = envdata[,-1]
season = list()
for(i in 1:3){
    season[[i]] = subset(envdata, clust == i)
}
env_mean = map(season, apply, MARGIN = 2, FUN = mean)
```

### Show environmental variables according to different season
```{r}
kable(data.frame(env_mean), col.names = c("Spring", "Summer", "Winter"), digits = 2)
```

It can be observed that **spring** has a lower *depth*, *temperature* and higher *dissolved oxygen* level. Next, **summer** comes with higher *temperature*, *fluorescence*, *fish density*,  *copepod density* and lower *dissolved oxygen* level. Finally, **winter** results in higher *depth*, lower *fish and copepod density*. The levels of *salinity* do not vary much across different seasons.

How exactly does these environmental variables affect each specific dominant copepod species would require other data or experiments to determine.